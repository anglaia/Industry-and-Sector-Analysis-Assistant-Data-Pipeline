"""
AI RAG å¿«é€Ÿæ„å»ºå·¥å…·ï¼ˆç®€åŒ–ç‰ˆï¼‰
ä¸“æ³¨äºAIè¡Œä¸šæ–‡ç« çš„é‡‡é›†å’Œå‘é‡åŒ–

ä½¿ç”¨ç¤ºä¾‹:
    python run_ai_rag.py --articles 10
    python run_ai_rag.py --articles 20 --no-headless  # æ˜¾ç¤ºæµè§ˆå™¨
    python run_ai_rag.py --preview-only  # åªé¢„è§ˆä¸ä¸Šä¼ 
"""
import argparse
from pathlib import Path
from datetime import datetime
from scrapers.mckinsey_playwright_scraper import McKinseyPlaywrightScraper
from ingest.batch_processor import BatchProcessor
from utils.logger import logger
from config.settings import settings
import json

# AI å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
AI_KEYWORDS = [
    "artificial intelligence", "AI", "machine learning", "deep learning",
    "neural network", "generative AI", "large language model", "LLM",
    "GPT", "transformer", "computer vision", "natural language processing",
    "reinforcement learning", "AI ethics", "AI strategy", "AI implementation"
]


def is_ai_relevant(text: str, threshold: int = 2) -> bool:
    """
    æ£€æŸ¥æ–‡ç« æ˜¯å¦ä¸AIç›¸å…³
    
    Args:
        text: æ–‡ç« æ–‡æœ¬
        threshold: è‡³å°‘åŒ…å«å¤šå°‘ä¸ªAIå…³é”®è¯
        
    Returns:
        æ˜¯å¦ä¸AIç›¸å…³
    """
    if not text:
        return False
    
    text_lower = text.lower()
    count = sum(1 for keyword in AI_KEYWORDS if keyword.lower() in text_lower)
    return count >= threshold


def filter_ai_articles(articles: list) -> list:
    """
    è¿‡æ»¤å‡ºä¸AIç›¸å…³çš„æ–‡ç« 
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
        
    Returns:
        è¿‡æ»¤åçš„æ–‡ç« åˆ—è¡¨
    """
    filtered = []
    
    for article in articles:
        # æ£€æŸ¥æ ‡é¢˜å’Œå†…å®¹
        title = article.get('title', '')
        content = article.get('content', '')
        full_text = f"{title} {content}"
        
        if is_ai_relevant(full_text):
            filtered.append(article)
            logger.info(f"âœ… AIç›¸å…³: {title[:60]}...")
        else:
            logger.debug(f"â­ï¸  è·³è¿‡éAIæ–‡ç« : {title[:60]}...")
    
    return filtered


def run_scraping(max_articles: int, headless: bool = True, ai_filter: bool = True):
    """
    è¿è¡Œçˆ¬è™«é‡‡é›†AIæ–‡ç« 
    
    Args:
        max_articles: æœ€å¤§æ–‡ç« æ•°
        headless: æ˜¯å¦æ— å¤´æ¨¡å¼
        ai_filter: æ˜¯å¦å¯ç”¨AIè¿‡æ»¤
        
    Returns:
        é‡‡é›†åˆ°çš„æ–‡ç« åˆ—è¡¨
    """
    logger.info("=" * 70)
    logger.info("ğŸ” æ­¥éª¤ 1/2: å¼€å§‹çˆ¬å–McKinsey AIæ–‡ç« ")
    logger.info("=" * 70)
    
    scraper = McKinseyPlaywrightScraper(headless=headless)
    
    try:
        # çˆ¬å–æ–‡ç« 
        articles = scraper.scrape(max_items=max_articles)
        
        if not articles:
            logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ")
            return []
        
        logger.info(f"âœ… çˆ¬å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        # AIè¿‡æ»¤
        if ai_filter:
            logger.info("\nğŸ¯ åº”ç”¨AIå…³é”®è¯è¿‡æ»¤...")
            articles = filter_ai_articles(articles)
            logger.info(f"âœ… è¿‡æ»¤åå‰©ä½™ {len(articles)} ç¯‡AIç›¸å…³æ–‡ç« ")
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = settings.processed_files_path / f"mckinsey_ai_{timestamp}.json"
        scraper.save_results(articles, output_file)
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file.name}")
        
        return articles
        
    finally:
        scraper.close()


def run_ingestion(articles: list, preview_only: bool = False):
    """
    è¿è¡Œå‘é‡åŒ–å¹¶ä¸Šä¼ åˆ°Pinecone
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
        preview_only: ä»…é¢„è§ˆï¼Œä¸å®é™…ä¸Šä¼ 
        
    Returns:
        æ‘„å–ç»“æœ
    """
    if not articles:
        logger.warning("âš ï¸  æ²¡æœ‰æ–‡ç« éœ€è¦å¤„ç†")
        return None
    
    logger.info("\n" + "=" * 70)
    logger.info("ğŸš€ æ­¥éª¤ 2/2: å‘é‡åŒ–å¤„ç†å¹¶ä¸Šä¼ åˆ°Pinecone")
    logger.info("=" * 70)
    
    if preview_only:
        logger.info("ğŸ“‹ é¢„è§ˆæ¨¡å¼ - åªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼Œä¸ä¸Šä¼ ")
        total_chars = sum(len(a.get('content', '')) for a in articles)
        logger.info(f"ğŸ“Š ç»Ÿè®¡:")
        logger.info(f"   - æ–‡ç« æ•°: {len(articles)}")
        logger.info(f"   - æ€»å­—ç¬¦æ•°: {total_chars:,}")
        logger.info(f"   - é¢„è®¡chunks: ~{total_chars // 2000}")
        return None
    
    # å‡†å¤‡æ–‡æ¡£æ•°æ®
    documents = []
    for i, article in enumerate(articles, 1):
        content = article.get('content', '')
        if not content or len(content) < 200:
            logger.warning(f"â­ï¸  è·³è¿‡å†…å®¹è¿‡çŸ­çš„æ–‡ç« : {article.get('title', 'Unknown')[:50]}")
            continue
        
        doc_data = {
            'text': content,
            'file_id': f"mckinsey_ai_{datetime.now().strftime('%Y%m%d')}_{i}",
            'industry': 'AI',
            'metadata': {
                'title': article.get('title', ''),
                'url': article.get('url', ''),
                'author': article.get('author', ''),
                'date': article.get('date', ''),
                'source': 'McKinsey AI',
                'collection_date': datetime.now().isoformat()
            }
        }
        documents.append(doc_data)
    
    if not documents:
        logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å¯ä»¥å¤„ç†")
        return None
    
    logger.info(f"ğŸ“¦ å‡†å¤‡å¤„ç† {len(documents)} ç¯‡æ–‡ç« ...")
    
    # æ‰¹é‡æ‘„å–
    try:
        processor = BatchProcessor()
        result = processor.ingester.ingest_batch(documents)
        
        logger.info("\n" + "=" * 70)
        logger.info("âœ… å®Œæˆï¼RAGåº“å·²æ›´æ–°")
        logger.info("=" * 70)
        logger.info(f"ğŸ“Š æ‘„å–ç»Ÿè®¡:")
        logger.info(f"   - æ€»æ–‡ç« æ•°: {result['total']}")
        logger.info(f"   - æˆåŠŸ: {result['successful']}")
        logger.info(f"   - å¤±è´¥: {result['failed']}")
        logger.info(f"   - æ€»chunks: {result['total_chunks']}")
        logger.info(f"\nğŸ‰ ä½ ç°åœ¨å¯ä»¥åœ¨backend-aiä¸­æŸ¥è¯¢è¿™äº›AIè¡Œä¸šå†…å®¹äº†!")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ‘„å–å¤±è´¥: {e}", exc_info=True)
        return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="AI RAG å¿«é€Ÿæ„å»ºå·¥å…· - ä¸“æ³¨äºAIè¡Œä¸šæ–‡ç« é‡‡é›†å’Œå‘é‡åŒ–",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # é‡‡é›†10ç¯‡AIæ–‡ç« å¹¶ä¸Šä¼ 
  python run_ai_rag.py --articles 10
  
  # é‡‡é›†20ç¯‡æ–‡ç« ï¼Œæ˜¾ç¤ºæµè§ˆå™¨ï¼ˆè°ƒè¯•ç”¨ï¼‰
  python run_ai_rag.py --articles 20 --no-headless
  
  # åªé¢„è§ˆä¸ä¸Šä¼ 
  python run_ai_rag.py --articles 5 --preview-only
  
  # å…³é—­AIè¿‡æ»¤ï¼Œé‡‡é›†æ‰€æœ‰æ–‡ç« 
  python run_ai_rag.py --articles 15 --no-filter
        """
    )
    
    parser.add_argument(
        '--articles',
        type=int,
        default=10,
        help='é‡‡é›†æ–‡ç« æ•°é‡ (é»˜è®¤: 10)'
    )
    
    parser.add_argument(
        '--no-headless',
        action='store_true',
        help='æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼ˆç”¨äºè°ƒè¯•ï¼‰'
    )
    
    parser.add_argument(
        '--preview-only',
        action='store_true',
        help='åªé¢„è§ˆç»Ÿè®¡ä¿¡æ¯ï¼Œä¸ä¸Šä¼ åˆ°Pinecone'
    )
    
    parser.add_argument(
        '--no-filter',
        action='store_true',
        help='å…³é—­AIå…³é”®è¯è¿‡æ»¤ï¼Œé‡‡é›†æ‰€æœ‰McKinseyæ–‡ç« '
    )
    
    args = parser.parse_args()
    
    # éªŒè¯APIå¯†é’¥
    if not settings.validate_required_keys():
        logger.error("âŒ è¯·å…ˆé…ç½® PINECONE_API_KEY å’Œ GOOGLE_API_KEY")
        return 1
    
    # æ˜¾ç¤ºé…ç½®
    logger.info("=" * 70)
    logger.info("ğŸ¤– AI RAG å¿«é€Ÿæ„å»ºå·¥å…·")
    logger.info("=" * 70)
    logger.info(f"ğŸ“ é…ç½®:")
    logger.info(f"   - é‡‡é›†æ•°é‡: {args.articles} ç¯‡")
    logger.info(f"   - æµè§ˆå™¨æ¨¡å¼: {'å¯è§' if not args.no_headless else 'æ— å¤´'}")
    logger.info(f"   - AIè¿‡æ»¤: {'å¼€å¯' if not args.no_filter else 'å…³é—­'}")
    logger.info(f"   - é¢„è§ˆæ¨¡å¼: {'æ˜¯' if args.preview_only else 'å¦'}")
    logger.info(f"   - Pineconeç´¢å¼•: {settings.pinecone_index_name}")
    logger.info("")
    
    try:
        # æ­¥éª¤1: çˆ¬å–æ–‡ç« 
        articles = run_scraping(
            max_articles=args.articles,
            headless=not args.no_headless,
            ai_filter=not args.no_filter
        )
        
        if not articles:
            logger.error("âŒ æœªé‡‡é›†åˆ°ä»»ä½•æ–‡ç« ï¼Œé€€å‡º")
            return 1
        
        # æ­¥éª¤2: å‘é‡åŒ–å¹¶ä¸Šä¼ 
        result = run_ingestion(articles, preview_only=args.preview_only)
        
        if result or args.preview_only:
            return 0
        else:
            return 1
            
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())

