"""
ä» S3 å¯¼å…¥ PDF æ–‡æ¡£åˆ° Pinecone å‘é‡æ•°æ®åº“
ç”¨äºæ”¯æŒ RAG ç³»ç»Ÿçš„ Citations åŠŸèƒ½

ä½¿ç”¨ç¤ºä¾‹:
    python import_s3_to_pinecone.py --bucket your-bucket-name --prefix reports/
    python import_s3_to_pinecone.py --preview-only  # åªé¢„è§ˆæ–‡ä»¶åˆ—è¡¨
    python import_s3_to_pinecone.py --max-files 10  # é™åˆ¶å¯¼å…¥æ–‡ä»¶æ•°
"""
import os
import argparse
import boto3
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
import tempfile
from dotenv import load_dotenv

from ingest.batch_processor import BatchProcessor
from processors.pdf_processor import PDFProcessor
from utils.logger import logger
from config.settings import settings

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class S3PDFImporter:
    """ä» S3 å¯¼å…¥ PDF æ–‡æ¡£åˆ° Pinecone"""
    
    def __init__(
        self, 
        bucket_name: Optional[str] = None,
        region: Optional[str] = None,
        access_key: Optional[str] = None,
        secret_key: Optional[str] = None
    ):
        """
        åˆå§‹åŒ– S3 å®¢æˆ·ç«¯
        
        Args:
            bucket_name: S3 bucket åç§°ï¼ˆå¦‚ä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            region: AWS regionï¼ˆå¦‚ä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            access_key: AWS Access Keyï¼ˆå¦‚ä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            secret_key: AWS Secret Keyï¼ˆå¦‚ä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        """
        # ä»å‚æ•°æˆ–ç¯å¢ƒå˜é‡è·å–é…ç½®
        # å…¼å®¹ä¸¤å¥—å‘½åï¼š
        # - data-pipeline: AWS_S3_BUCKET_NAME / AWS_REGION
        # - æ ¹ç›®å½•ç®¡çº¿: S3_BUCKET_NAME / S3_REGION
        self.bucket_name = bucket_name or os.getenv("AWS_S3_BUCKET_NAME") or os.getenv("S3_BUCKET_NAME")
        region = region or os.getenv("AWS_REGION") or os.getenv("S3_REGION") or "us-east-1"
        access_key = access_key or os.getenv('AWS_ACCESS_KEY_ID')
        secret_key = secret_key or os.getenv('AWS_SECRET_ACCESS_KEY')
        
        if not self.bucket_name:
            raise ValueError("å¿…é¡»æä¾› bucket_name æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ AWS_S3_BUCKET_NAME")
        
        if not access_key or not secret_key:
            raise ValueError("å¿…é¡»æä¾› AWS credentials æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ AWS_ACCESS_KEY_ID å’Œ AWS_SECRET_ACCESS_KEY")
        
        # åˆå§‹åŒ– S3 å®¢æˆ·ç«¯
        self.s3_client = boto3.client(
            's3',
            region_name=region,
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key
        )
        
        # åˆå§‹åŒ– PDF å¤„ç†å™¨
        self.pdf_processor = PDFProcessor()
        
        logger.info(f"âœ… S3 å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {self.bucket_name} ({region})")
    
    def list_pdf_files(self, prefix: str = '', max_files: Optional[int] = None) -> List[Dict]:
        """
        åˆ—å‡º S3 bucket ä¸­çš„ PDF æ–‡ä»¶
        
        Args:
            prefix: S3 key å‰ç¼€ï¼ˆç±»ä¼¼æ–‡ä»¶å¤¹è·¯å¾„ï¼‰
            max_files: æœ€å¤§æ–‡ä»¶æ•°é™åˆ¶
            
        Returns:
            æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f"ğŸ” æ­£åœ¨æ‰«æ S3 bucket: {self.bucket_name}/{prefix}")
        
        pdf_files = []
        paginator = self.s3_client.get_paginator('list_objects_v2')
        
        try:
            for page in paginator.paginate(Bucket=self.bucket_name, Prefix=prefix):
                if 'Contents' not in page:
                    continue
                
                for obj in page['Contents']:
                    key = obj['Key']
                    
                    # åªå¤„ç† PDF æ–‡ä»¶
                    if key.lower().endswith('.pdf'):
                        pdf_files.append({
                            'key': key,
                            'size': obj['Size'],
                            'last_modified': obj['LastModified'],
                            'filename': Path(key).name
                        })
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ–‡ä»¶æ•°
                        if max_files and len(pdf_files) >= max_files:
                            logger.info(f"âš ï¸  å·²è¾¾åˆ°æœ€å¤§æ–‡ä»¶æ•°é™åˆ¶: {max_files}")
                            break
                
                if max_files and len(pdf_files) >= max_files:
                    break
            
            logger.info(f"âœ… æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
            return pdf_files
            
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            return []
    
    def download_and_process_pdf(self, s3_key: str) -> Optional[Dict]:
        """
        ä» S3 ä¸‹è½½ PDF å¹¶æå–æ–‡æœ¬
        
        Args:
            s3_key: S3 å¯¹è±¡çš„ key
            
        Returns:
            å¤„ç†åçš„æ–‡æ¡£æ•°æ®ï¼ŒåŒ…å«æ–‡æœ¬å†…å®¹å’Œå…ƒæ•°æ®
        """
        temp_file = None
        
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as tmp:
                temp_file = tmp.name
                
                # ä» S3 ä¸‹è½½æ–‡ä»¶
                logger.debug(f"ğŸ“¥ ä¸‹è½½ä¸­: {s3_key}")
                self.s3_client.download_file(self.bucket_name, s3_key, temp_file)
                
                # æå– PDF æ–‡æœ¬
                text = self.pdf_processor.extract_text(temp_file)
                
                if not text or len(text.strip()) < 100:
                    logger.warning(f"âš ï¸  æ–‡ä»¶å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º: {s3_key}")
                    return None
                
                # æ„å»º S3 URL
                s3_url = f"https://{self.bucket_name}.s3.amazonaws.com/{s3_key}"
                
                # è·å–æ–‡ä»¶åå’Œå…ƒæ•°æ®
                filename = Path(s3_key).name
                
                # å°è¯•ä»æ–‡ä»¶åæ¨æ–­è¡Œä¸šç±»åˆ«
                industry = self._infer_industry(filename, text)
                
                doc_data = {
                    'text': text,
                    'file_id': s3_key.replace('/', '_').replace('.pdf', ''),
                    'industry': industry,
                    'metadata': {
                        'source_file': filename,
                        's3_url': s3_url,
                        's3_key': s3_key,
                        'bucket': self.bucket_name,
                        'file_size': os.path.getsize(temp_file),
                        'ingestion_date': datetime.now().isoformat(),
                        'source_type': 's3_import'
                    }
                }
                
                logger.debug(f"âœ… å¤„ç†å®Œæˆ: {filename} ({len(text)} å­—ç¬¦)")
                return doc_data
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {s3_key}: {e}", exc_info=True)
            return None
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file and os.path.exists(temp_file):
                try:
                    os.unlink(temp_file)
                except:
                    pass
    
    def _infer_industry(self, filename: str, text: str) -> str:
        """
        ä»æ–‡ä»¶åæˆ–å†…å®¹æ¨æ–­è¡Œä¸šç±»åˆ«
        
        Args:
            filename: æ–‡ä»¶å
            text: æ–‡æœ¬å†…å®¹ï¼ˆå–å‰1000å­—ç¬¦ï¼‰
            
        Returns:
            è¡Œä¸šç±»åˆ«
        """
        # è¡Œä¸šå…³é”®è¯æ˜ å°„
        industry_keywords = {
            'Technology': ['tech', 'software', 'ai', 'artificial intelligence', 'cloud', 'saas', 'digital'],
            'Healthcare': ['health', 'medical', 'pharma', 'biotech', 'clinical', 'patient'],
            'Finance': ['finance', 'banking', 'fintech', 'investment', 'insurance', 'trading'],
            'Energy': ['energy', 'oil', 'gas', 'renewable', 'solar', 'wind', 'power'],
            'Manufacturing': ['manufacturing', 'factory', 'production', 'supply chain', 'automotive'],
            'Retail': ['retail', 'ecommerce', 'consumer', 'shopping', 'merchandise'],
            'Education': ['education', 'learning', 'university', 'school', 'academic'],
        }
        
        # åˆå¹¶æ–‡ä»¶åå’Œæ–‡æœ¬å‰1000å­—ç¬¦ç”¨äºåˆ†æ
        combined_text = f"{filename.lower()} {text[:1000].lower()}"
        
        # æŸ¥æ‰¾åŒ¹é…çš„è¡Œä¸š
        for industry, keywords in industry_keywords.items():
            if any(keyword in combined_text for keyword in keywords):
                return industry
        
        return 'General'  # é»˜è®¤ç±»åˆ«
    
    def import_to_pinecone(
        self, 
        prefix: str = '', 
        max_files: Optional[int] = None,
        preview_only: bool = False
    ) -> Dict:
        """
        æ‰§è¡Œå¯¼å…¥æµç¨‹
        
        Args:
            prefix: S3 key å‰ç¼€
            max_files: æœ€å¤§æ–‡ä»¶æ•°
            preview_only: ä»…é¢„è§ˆä¸å¯¼å…¥
            
        Returns:
            å¯¼å…¥ç»“æœç»Ÿè®¡
        """
        # æ­¥éª¤1: åˆ—å‡ºæ–‡ä»¶
        pdf_files = self.list_pdf_files(prefix, max_files)
        
        if not pdf_files:
            logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½• PDF æ–‡ä»¶")
            return {'total': 0, 'successful': 0, 'failed': 0}
        
        # é¢„è§ˆæ¨¡å¼
        if preview_only:
            logger.info("\n" + "=" * 70)
            logger.info("ğŸ“‹ é¢„è§ˆæ¨¡å¼ - æ–‡ä»¶åˆ—è¡¨")
            logger.info("=" * 70)
            
            for i, file_info in enumerate(pdf_files, 1):
                logger.info(f"{i}. {file_info['filename']}")
                logger.info(f"   è·¯å¾„: {file_info['key']}")
                logger.info(f"   å¤§å°: {file_info['size']:,} bytes")
                logger.info(f"   ä¿®æ”¹æ—¶é—´: {file_info['last_modified']}")
                logger.info("")
            
            total_size = sum(f['size'] for f in pdf_files)
            logger.info(f"ğŸ“Š ç»Ÿè®¡: {len(pdf_files)} ä¸ªæ–‡ä»¶, æ€»å¤§å°: {total_size:,} bytes")
            return {'total': len(pdf_files), 'successful': 0, 'failed': 0}
        
        # æ­¥éª¤2: ä¸‹è½½å¹¶å¤„ç†æ‰€æœ‰ PDF
        logger.info("\n" + "=" * 70)
        logger.info(f"ğŸ“¥ æ­¥éª¤ 1/2: ä¸‹è½½å¹¶å¤„ç† PDF æ–‡ä»¶")
        logger.info("=" * 70)
        
        documents = []
        failed_count = 0
        
        for i, file_info in enumerate(pdf_files, 1):
            logger.info(f"[{i}/{len(pdf_files)}] å¤„ç†ä¸­: {file_info['filename']}")
            
            doc_data = self.download_and_process_pdf(file_info['key'])
            
            if doc_data:
                documents.append(doc_data)
            else:
                failed_count += 1
        
        if not documents:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶")
            return {'total': len(pdf_files), 'successful': 0, 'failed': failed_count}
        
        logger.info(f"âœ… æˆåŠŸå¤„ç† {len(documents)} ä¸ªæ–‡ä»¶, å¤±è´¥ {failed_count} ä¸ª")
        
        # æ­¥éª¤3: æ‰¹é‡å¯¼å…¥åˆ° Pinecone
        logger.info("\n" + "=" * 70)
        logger.info("ğŸš€ æ­¥éª¤ 2/2: å‘é‡åŒ–å¹¶ä¸Šä¼ åˆ° Pinecone")
        logger.info("=" * 70)
        
        try:
            processor = BatchProcessor()
            result = processor.ingester.ingest_batch(documents)
            
            logger.info("\n" + "=" * 70)
            logger.info("âœ… å¯¼å…¥å®Œæˆï¼")
            logger.info("=" * 70)
            logger.info(f"ğŸ“Š å¯¼å…¥ç»Ÿè®¡:")
            logger.info(f"   - æ€»æ–‡ä»¶æ•°: {result['total']}")
            logger.info(f"   - æˆåŠŸ: {result['successful']}")
            logger.info(f"   - å¤±è´¥: {result['failed']}")
            logger.info(f"   - æ€» chunks: {result['total_chunks']}")
            logger.info(f"\nğŸ‰ ç°åœ¨ä½ å¯ä»¥åœ¨å‰ç«¯çœ‹åˆ°å¸¦ Citations çš„å†…å®¹äº†!")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥åˆ° Pinecone å¤±è´¥: {e}", exc_info=True)
            return {'total': len(documents), 'successful': 0, 'failed': len(documents)}


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä» S3 å¯¼å…¥ PDF æ–‡æ¡£åˆ° Pinecone å‘é‡æ•°æ®åº“",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å¯¼å…¥æ‰€æœ‰ PDF
  python import_s3_to_pinecone.py
  
  # å¯¼å…¥æŒ‡å®šå‰ç¼€ï¼ˆæ–‡ä»¶å¤¹ï¼‰çš„ PDF
  python import_s3_to_pinecone.py --prefix reports/2024/
  
  # åªé¢„è§ˆæ–‡ä»¶åˆ—è¡¨
  python import_s3_to_pinecone.py --preview-only
  
  # é™åˆ¶å¯¼å…¥æ–‡ä»¶æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
  python import_s3_to_pinecone.py --max-files 5
  
  # æŒ‡å®š bucketï¼ˆä¸ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
  python import_s3_to_pinecone.py --bucket my-bucket-name
        """
    )
    
    parser.add_argument(
        '--bucket',
        type=str,
        help='S3 bucket åç§°ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡ AWS_S3_BUCKET_NAME è¯»å–ï¼‰'
    )
    
    parser.add_argument(
        '--prefix',
        type=str,
        default='',
        help='S3 key å‰ç¼€ï¼ˆç±»ä¼¼æ–‡ä»¶å¤¹è·¯å¾„ï¼Œä¾‹å¦‚: reports/2024/ï¼‰'
    )
    
    parser.add_argument(
        '--max-files',
        type=int,
        help='æœ€å¤§å¯¼å…¥æ–‡ä»¶æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰'
    )
    
    parser.add_argument(
        '--preview-only',
        action='store_true',
        help='åªé¢„è§ˆæ–‡ä»¶åˆ—è¡¨ï¼Œä¸å®é™…å¯¼å…¥'
    )
    
    args = parser.parse_args()
    
    # éªŒè¯é…ç½®
    if not settings.validate_required_keys():
        logger.error("âŒ è¯·å…ˆé…ç½® PINECONE_API_KEY å’Œ GOOGLE_API_KEY")
        logger.error("   å¯ä»¥åœ¨ backend-ai/.env æˆ– data-pipeline/.env ä¸­é…ç½®")
        return 1
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    bucket_name = args.bucket or os.getenv('AWS_S3_BUCKET_NAME', '<æœªè®¾ç½®>')
    
    logger.info("=" * 70)
    logger.info("ğŸ“¦ S3 åˆ° Pinecone å¯¼å…¥å·¥å…·")
    logger.info("=" * 70)
    logger.info(f"ğŸ“ é…ç½®:")
    logger.info(f"   - S3 Bucket: {bucket_name}")
    logger.info(f"   - å‰ç¼€: {args.prefix or '(æ‰€æœ‰æ–‡ä»¶)'}")
    logger.info(f"   - æœ€å¤§æ–‡ä»¶æ•°: {args.max_files or '(æ— é™åˆ¶)'}")
    logger.info(f"   - é¢„è§ˆæ¨¡å¼: {'æ˜¯' if args.preview_only else 'å¦'}")
    logger.info(f"   - Pinecone ç´¢å¼•: {settings.pinecone_index_name}")
    logger.info("")
    
    try:
        # åˆå§‹åŒ–å¯¼å…¥å™¨
        importer = S3PDFImporter(bucket_name=args.bucket)
        
        # æ‰§è¡Œå¯¼å…¥
        result = importer.import_to_pinecone(
            prefix=args.prefix,
            max_files=args.max_files,
            preview_only=args.preview_only
        )
        
        if result['total'] > 0 and (result['successful'] > 0 or args.preview_only):
            return 0
        else:
            return 1
            
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())

