"""
McKinsey Insights çˆ¬è™« (Playwright ç‰ˆæœ¬ - ä¸¤é˜¶æ®µæ¨¡å¼)
é˜¶æ®µä¸€ï¼šæ”¶å‰²æœº - æ”¶é›†æ‰€æœ‰æ–‡ç« URL
é˜¶æ®µäºŒï¼šåŠ å·¥å‚ - çˆ¬å–æ¯ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹
"""
from typing import List, Dict, Optional
from pathlib import Path
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import time
import re
from .playwright_base_scraper import PlaywrightBaseScraper
from utils.logger import logger
from utils.file_utils import FileUtils
from config.settings import settings
from datetime import datetime


class McKinseyPlaywrightScraper(PlaywrightBaseScraper):
    """
    McKinsey Insights çˆ¬è™«ï¼ˆPlaywright ç‰ˆæœ¬ï¼‰
    
    ä½¿ç”¨çœŸå®æµè§ˆå™¨æ¥ç»•è¿‡åçˆ¬è™«æœºåˆ¶
    æ³¨æ„ï¼šæ­¤çˆ¬è™«ä»…ç”¨äºæ•™è‚²å’Œç ”ç©¶ç›®çš„
    è¯·éµå®ˆç½‘ç«™çš„ robots.txt å’ŒæœåŠ¡æ¡æ¬¾
    """
    
    def __init__(self, headless: bool = True):
        super().__init__(
            name="McKinsey Insights (Playwright)",
            base_url="https://www.mckinsey.com",
            delay=3.0,  # ç¤¼è²Œæ€§å»¶è¿Ÿ
            headless=headless,
            browser_type="chromium"
        )
        
        # è¾“å‡ºç›®å½•
        self.output_dir = settings.raw_files_path / "mckinsey_reports"
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def scrape(self, max_items: Optional[int] = 10) -> List[Dict]:
        """
        ä¸¤é˜¶æ®µçˆ¬å– McKinsey æŠ¥å‘Š
        
        é˜¶æ®µä¸€ï¼šæ”¶é›†æ–‡ç« URLåˆ—è¡¨
        é˜¶æ®µäºŒï¼šçˆ¬å–æ¯ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹
        
        Args:
            max_items: æœ€å¤§çˆ¬å–æ•°é‡
            
        Returns:
            æŠ¥å‘Šä¿¡æ¯åˆ—è¡¨
        """
        # å¯åŠ¨æµè§ˆå™¨
        if not self.page:
            self.start()
        
        # ğŸŒ¾ é˜¶æ®µä¸€ï¼šæ”¶å‰²æœº - æ”¶é›†æ–‡ç« URL
        logger.info("=" * 60)
        logger.info("ğŸŒ¾ é˜¶æ®µä¸€ï¼šæ”¶å‰²æœº - æ”¶é›†æ–‡ç« URLåˆ—è¡¨")
        logger.info("=" * 60)
        
        article_urls = self._collect_article_urls(max_items)
        
        if not article_urls:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
            return []
        
        logger.info(f"âœ… æ”¶é›†åˆ° {len(article_urls)} ä¸ªæ–‡ç« é“¾æ¥")
        
        # ğŸ­ é˜¶æ®µäºŒï¼šåŠ å·¥å‚ - çˆ¬å–å®Œæ•´å†…å®¹
        logger.info("")
        logger.info("=" * 60)
        logger.info("ğŸ­ é˜¶æ®µäºŒï¼šåŠ å·¥å‚ - çˆ¬å–æ–‡ç« å®Œæ•´å†…å®¹")
        logger.info("=" * 60)
        
        results = []
        for i, url in enumerate(article_urls, 1):
            logger.info(f"\nğŸ“– [{i}/{len(article_urls)}] å¤„ç†æ–‡ç« : {url}")
            
            article_data = self._scrape_article_detail(url)
            if article_data:
                results.append(article_data)
                logger.info(f"âœ… æˆåŠŸçˆ¬å–: {article_data['title'][:50]}...")
            else:
                logger.warning(f"âš ï¸  è·³è¿‡æ–‡ç« : {url}")
            
            # ç¤¼è²Œæ€§å»¶è¿Ÿ
            if i < len(article_urls):
                time.sleep(self.delay)
        
        logger.info("")
        logger.info("=" * 60)
        logger.info(f"ğŸ‰ å®Œæˆï¼æˆåŠŸçˆ¬å– {len(results)}/{len(article_urls)} ç¯‡æ–‡ç« ")
        logger.info("=" * 60)
        
        return results
    
    def _collect_article_urls(self, max_items: int) -> List[str]:
        """
        é˜¶æ®µä¸€ï¼šä»åˆ—è¡¨é¡µæ”¶é›†æ–‡ç« URL
        
        Args:
            max_items: æœ€å¤§æ”¶é›†æ•°é‡
            
        Returns:
            æ–‡ç« URLåˆ—è¡¨
        """
        insights_url = urljoin(self.base_url, "/featured-insights")
        
        logger.info(f"â³ è®¿é—®åˆ—è¡¨é¡µ: {insights_url}")
        
        # è®¿é—®åˆ—è¡¨é¡µ
        if not self.get_page(insights_url):
            logger.error("âŒ æ— æ³•åŠ è½½åˆ—è¡¨é¡µ")
            return []
        
        logger.info("âœ… åˆ—è¡¨é¡µåŠ è½½æˆåŠŸ")
        
        # ğŸª å¤„ç† Cookie å¼¹çª—
        logger.info("ğŸª æ£€æŸ¥å¹¶å¤„ç† Cookie å¼¹çª—...")
        self.handle_cookie_popup()
        time.sleep(2)
        
        # ğŸ“œ æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
        logger.info("ğŸ“œ æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šæ–‡ç« ...")
        self.scroll_page(scroll_times=5)
        time.sleep(2)
        
        # ä¿å­˜è°ƒè¯•ä¿¡æ¯
        screenshot_path = self.output_dir / "stage1_article_list.png"
        self.take_screenshot(screenshot_path)
        logger.info(f"ğŸ“¸ ä¿å­˜æˆªå›¾: {screenshot_path}")
        
        # è·å– HTML
        html = self.get_html()
        if not html or len(html) < 1000:
            logger.error(f"âŒ é¡µé¢å†…å®¹å¼‚å¸¸: {len(html)} bytes")
            return []
        
        # ä¿å­˜ HTML ç”¨äºè°ƒè¯•
        html_path = self.output_dir / "stage1_page.html"
        html_path.write_text(html, encoding='utf-8')
        logger.info(f"ğŸ’¾ ä¿å­˜HTML: {html_path}")
        
        # è§£ææ–‡ç« é“¾æ¥
        soup = BeautifulSoup(html, 'lxml')
        article_urls = self._extract_article_urls(soup, max_items)
        
        return article_urls
    
    def _extract_article_urls(self, soup: BeautifulSoup, max_items: int) -> List[str]:
        """
        ä»é¡µé¢ä¸­æå–æ–‡ç« è¯¦æƒ…é¡µURL
        
        Args:
            soup: BeautifulSoup å¯¹è±¡
            max_items: æœ€å¤§æå–æ•°é‡
            
        Returns:
            æ–‡ç« URLåˆ—è¡¨
        """
        urls = set()
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        all_links = soup.find_all('a', href=True)
        logger.info(f"ğŸ” é¡µé¢ä¸­å…±æœ‰ {len(all_links)} ä¸ªé“¾æ¥")
        
        for link in all_links:
            href = link.get('href', '')
            
            # è¿‡æ»¤å‡ºæ–‡ç« é“¾æ¥
            # McKinsey æ–‡ç« URLé€šå¸¸åŒ…å«è¿™äº›æ¨¡å¼
            if any(pattern in href for pattern in [
                '/featured-insights/',
                '/our-insights/',
                '/industries/',
                '/business-functions/',
                '/capabilities/'
            ]):
                # æ’é™¤éæ–‡ç« é“¾æ¥
                if any(skip in href for skip in [
                    '#', 'javascript:', 'mailto:', 
                    '.pdf', '.jpg', '.png', '.svg',
                    '/search', '/subscribe', '/careers'
                ]):
                    continue
                
                # æ„å»ºå®Œæ•´URL
                full_url = urljoin(self.base_url, href)
                
                # ç¡®ä¿æ˜¯ McKinsey åŸŸå
                if 'mckinsey.com' in full_url:
                    urls.add(full_url)
        
        # è½¬ä¸ºåˆ—è¡¨å¹¶é™åˆ¶æ•°é‡
        url_list = list(urls)[:max_items * 3]  # å¤šæ”¶é›†ä¸€äº›ï¼Œå› ä¸ºæœ‰äº›å¯èƒ½æ— æ•ˆ
        
        logger.info(f"âœ… æ‰¾åˆ° {len(url_list)} ä¸ªå€™é€‰æ–‡ç« é“¾æ¥")
        
        # åªè¿”å›éœ€è¦çš„æ•°é‡
        return url_list[:max_items]
    
    def _scrape_article_detail(self, url: str) -> Optional[Dict]:
        """
        é˜¶æ®µäºŒï¼šçˆ¬å–å•ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹
        
        Args:
            url: æ–‡ç« è¯¦æƒ…é¡µURL
            
        Returns:
            æ¸…æ´—åçš„æ–‡ç« æ•°æ®
        """
        try:
            # è®¿é—®æ–‡ç« é¡µé¢
            if not self.get_page(url):
                logger.error(f"âŒ æ— æ³•åŠ è½½æ–‡ç« : {url}")
                return None
            
            # å¤„ç†å¯èƒ½çš„ Cookie å¼¹çª—
            self.handle_cookie_popup()
            time.sleep(1)
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            time.sleep(2)
            
            # è·å–HTML
            html = self.get_html()
            if not html or len(html) < 500:
                logger.error(f"âŒ æ–‡ç« å†…å®¹å¤ªçŸ­: {len(html)} bytes")
                return None
            
            # è§£æå†…å®¹
            soup = BeautifulSoup(html, 'lxml')
            
            # æå–å¹¶æ¸…æ´—æ•°æ®
            article_data = self._extract_and_clean_article(soup, url)
            
            return article_data
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–æ–‡ç« å¤±è´¥ {url}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return None
    
    def _extract_and_clean_article(self, soup: BeautifulSoup, url: str) -> Optional[Dict]:
        """
        æå–å¹¶æ¸…æ´—æ–‡ç« æ•°æ®
        
        Args:
            soup: BeautifulSoup å¯¹è±¡
            url: æ–‡ç« URL
            
        Returns:
            æ¸…æ´—åçš„æ–‡ç« æ•°æ®
        """
        try:
            # ğŸ·ï¸ æå–æ ‡é¢˜
            title = self._extract_title(soup)
            if not title or title == "No Title":
                logger.warning("âš ï¸  æœªæ‰¾åˆ°æ ‡é¢˜")
                return None
            
            # ğŸ“… æå–æ—¥æœŸ
            date = self._extract_date(soup)
            
            # âœï¸ æå–ä½œè€…
            authors = self._extract_authors(soup)
            
            # ğŸ“ æå–æ­£æ–‡ï¼ˆé‡ç‚¹ï¼ï¼‰
            content = self._extract_clean_content(soup)
            
            if not content or len(content) < 100:
                logger.warning(f"âš ï¸  æ­£æ–‡å†…å®¹å¤ªçŸ­: {len(content)} å­—ç¬¦")
                return None
            
            # ğŸ·ï¸ æå–æ ‡ç­¾
            tags = self._extract_tags(soup)
            
            # ğŸ“Š æ„å»ºæ•°æ®
            article_data = {
                'title': title,
                'url': url,
                'date': date,
                'authors': authors,
                'content': content,
                'tags': tags,
                'source': 'McKinsey Insights',
                'scraped_at': datetime.now().isoformat(),
                'content_length': len(content),
                'word_count': len(content.split())
            }
            
            logger.info(f"ğŸ“Š æ–‡ç« ç»Ÿè®¡: {len(content)} å­—ç¬¦, {article_data['word_count']} å•è¯")
            
            return article_data
            
        except Exception as e:
            logger.error(f"âŒ æå–æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–æ ‡é¢˜"""
        selectors = [
            'h1.article-title',
            'h1[class*="hero"]',
            'h1[class*="title"]',
            'h1',
            '[data-component*="headline"] h1',
            '[class*="headline"] h1'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True)
                if title and len(title) > 5:
                    return title
        
        return "No Title"
    
    def _extract_date(self, soup: BeautifulSoup) -> str:
        """æå–æ—¥æœŸ"""
        # æŸ¥æ‰¾ time æ ‡ç­¾
        time_elem = soup.find('time')
        if time_elem:
            return time_elem.get('datetime', time_elem.get_text(strip=True))
        
        # æŸ¥æ‰¾åŒ…å«æ—¥æœŸçš„å…ƒç´ 
        date_patterns = [
            r'\b\d{4}-\d{2}-\d{2}\b',
            r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}\b'
        ]
        
        text = soup.get_text()
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        return ""
    
    def _extract_authors(self, soup: BeautifulSoup) -> List[str]:
        """æå–ä½œè€…"""
        authors = []
        
        # å¸¸è§çš„ä½œè€…é€‰æ‹©å™¨
        author_selectors = [
            '[class*="author"]',
            '[data-component*="author"]',
            '.byline'
        ]
        
        for selector in author_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–å¤ªé•¿çš„æ–‡æœ¬
                if text and 3 < len(text) < 50 and 'by' not in text.lower():
                    authors.append(text)
        
        return list(set(authors))[:5]  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    def _extract_clean_content(self, soup: BeautifulSoup) -> str:
        """
        æå–å¹¶æ¸…æ´—æ­£æ–‡å†…å®¹
        
        è¿™æ˜¯æœ€é‡è¦çš„å‡½æ•°ï¼ç¡®ä¿æå–é«˜è´¨é‡çš„æ­£æ–‡
        """
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        # å°è¯•æŸ¥æ‰¾æ–‡ç« æ­£æ–‡å®¹å™¨
        content_selectors = [
            'article',
            '[class*="article-body"]',
            '[class*="content-body"]',
            '[data-component*="body"]',
            '[class*="rich-text"]',
            'main',
        ]
        
        content_container = None
        for selector in content_selectors:
            container = soup.select_one(selector)
            if container:
                content_container = container
                break
        
        if not content_container:
            # å¦‚æœæ‰¾ä¸åˆ°å®¹å™¨ï¼Œä½¿ç”¨æ•´ä¸ªbody
            content_container = soup.find('body')
        
        if not content_container:
            return ""
        
        # æå–æ‰€æœ‰æ®µè½
        paragraphs = []
        for p in content_container.find_all(['p', 'h2', 'h3', 'h4', 'li']):
            text = p.get_text(strip=True)
            
            # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ®µè½å’Œå¯¼èˆªæ–‡æœ¬
            if len(text) > 20 and not any(skip in text.lower() for skip in [
                'cookie', 'subscribe', 'sign up', 'download', 'share',
                'related', 'read more', 'learn more'
            ]):
                paragraphs.append(text)
        
        # åˆå¹¶æ®µè½
        content = '\n\n'.join(paragraphs)
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        content = re.sub(r'\n{3,}', '\n\n', content)
        content = re.sub(r' {2,}', ' ', content)
        
        return content.strip()
    
    def _extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """æå–æ ‡ç­¾/è¡Œä¸šåˆ†ç±»"""
        tags = []
        
        tag_selectors = [
            '[class*="tag"]',
            '[class*="topic"]',
            '[class*="category"]',
            '[data-component*="tag"]'
        ]
        
        for selector in tag_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                if text and 2 < len(text) < 30:
                    tags.append(text)
        
        return list(set(tags))[:10]  # å»é‡å¹¶é™åˆ¶æ•°é‡


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    scraper = McKinseyPlaywrightScraper(headless=False)
    
    try:
        # çˆ¬å–å‰ 3 ç¯‡æ–‡ç« 
        results = scraper.scrape(max_items=3)
        
        # ä¿å­˜ç»“æœ
        output_file = settings.processed_files_path / f"mckinsey_clean_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        scraper.save_results(results, output_file)
        
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(results)} ç¯‡æ–‡ç« ")
        print(f"ğŸ“ ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºç»Ÿè®¡
        if results:
            total_chars = sum(r['content_length'] for r in results)
            total_words = sum(r['word_count'] for r in results)
            print(f"\nğŸ“Š æ€»è®¡: {total_chars:,} å­—ç¬¦, {total_words:,} å•è¯")
        
    finally:
        scraper.close()
