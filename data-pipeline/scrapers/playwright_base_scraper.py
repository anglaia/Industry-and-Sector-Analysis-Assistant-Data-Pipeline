"""
åŸºäº Playwright çš„çˆ¬è™«åŸºç±»
ä½¿ç”¨çœŸå®æµè§ˆå™¨æ¥ç»•è¿‡åçˆ¬è™«æœºåˆ¶
"""
from abc import ABC, abstractmethod
from typing import List, Dict, Optional
from pathlib import Path
from playwright.sync_api import sync_playwright, Page, Browser, BrowserContext
import time
from utils.logger import logger
from utils.file_utils import FileUtils


class PlaywrightBaseScraper(ABC):
    """
    åŸºäº Playwright çš„åŸºç¡€çˆ¬è™«ç±»
    ä½¿ç”¨çœŸå®æµè§ˆå™¨æ¥çˆ¬å–ç½‘ç«™
    """
    
    def __init__(
        self,
        name: str,
        base_url: str,
        delay: float = 3.0,
        headless: bool = True,
        browser_type: str = "chromium"  # chromium, firefox, webkit
    ):
        """
        åˆå§‹åŒ– Playwright çˆ¬è™«
        
        Args:
            name: çˆ¬è™«åç§°
            base_url: åŸºç¡€ URL
            delay: è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
            browser_type: æµè§ˆå™¨ç±»å‹
        """
        self.name = name
        self.base_url = base_url
        self.delay = delay
        self.headless = headless
        self.browser_type = browser_type
        
        # Playwright å¯¹è±¡
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        
        logger.info(f"Initialized Playwright scraper: {self.name}")
    
    def start(self):
        """å¯åŠ¨æµè§ˆå™¨"""
        try:
            self.playwright = sync_playwright().start()
            
            # é€‰æ‹©æµè§ˆå™¨
            if self.browser_type == "firefox":
                browser_launcher = self.playwright.firefox
            elif self.browser_type == "webkit":
                browser_launcher = self.playwright.webkit
            else:
                browser_launcher = self.playwright.chromium
            
            # å¯åŠ¨æµè§ˆå™¨
            self.browser = browser_launcher.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                ]
            )
            
            # åˆ›å»ºä¸Šä¸‹æ–‡ï¼ˆæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ç¯å¢ƒï¼‰
            self.context = self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                locale='en-US',
                timezone_id='America/New_York',
                extra_http_headers={
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                }
            )
            
            # åˆ›å»ºé¡µé¢
            self.page = self.context.new_page()
            
            # éšè— webdriver ç‰¹å¾
            self.page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                
                // è¦†ç›– plugins å’Œ languages
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5]
                });
                
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['en-US', 'en']
                });
                
                // Chrome ç‰¹å¾
                window.chrome = {
                    runtime: {}
                };
            """)
            
            logger.info(f"Browser started: {self.browser_type}")
            
        except Exception as e:
            logger.error(f"Failed to start browser: {e}")
            raise
    
    def handle_cookie_popup(self, timeout: int = 5000):
        """
        å¤„ç†å¸¸è§çš„ Cookie å¼¹çª—
        
        Args:
            timeout: ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        """
        if not self.page:
            return
        
        # å¸¸è§çš„ Cookie åŒæ„æŒ‰é’®é€‰æ‹©å™¨
        cookie_selectors = [
            'button:has-text("Accept")',
            'button:has-text("Accept all")',
            'button:has-text("I agree")',
            'button:has-text("åŒæ„")',
            '#onetrust-accept-btn-handler',
            '#accept-recommended-btn-handler',
            '.cookie-consent-accept',
            '[aria-label*="Accept"]',
            '[id*="accept"]',
        ]
        
        for selector in cookie_selectors:
            try:
                button = self.page.locator(selector).first
                if button.is_visible(timeout=timeout):
                    logger.info(f"ğŸª Found cookie popup, clicking: {selector}")
                    button.click(timeout=timeout)
                    time.sleep(1)
                    logger.info("âœ… Cookie popup dismissed")
                    return
            except Exception:
                continue
        
        logger.debug("No cookie popup found (this is OK)")
    
    def get_page(self, url: str, wait_selector: Optional[str] = None, max_retries: int = 3, wait_for_network_idle: bool = False) -> bool:
        """
        è®¿é—®é¡µé¢
        
        Args:
            url: é¡µé¢ URL
            wait_selector: ç­‰å¾…çš„é€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            wait_for_network_idle: æ˜¯å¦ç­‰å¾…ç½‘ç»œç©ºé—²ï¼ˆé»˜è®¤ Falseï¼Œå› ä¸ºå¾ˆå¤šç½‘ç«™ä¼šä¸€ç›´æœ‰åå°è¯·æ±‚ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.page:
            self.start()
        
        for attempt in range(max_retries):
            try:
                logger.info(f"Navigating to {url} (attempt {attempt + 1}/{max_retries})")
                
                # å¯¼èˆªåˆ°é¡µé¢ï¼Œç­‰å¾… load äº‹ä»¶å³å¯
                response = self.page.goto(url, wait_until='load', timeout=60000)
                
                if response and response.status >= 400:
                    logger.warning(f"Got status {response.status} for {url}")
                    if attempt < max_retries - 1:
                        time.sleep(self.delay * (attempt + 1))
                        continue
                    return False
                
                logger.info(f"Page loaded with status {response.status if response else 'unknown'}")
                
                # ç­‰å¾…æŒ‡å®šé€‰æ‹©å™¨
                if wait_selector:
                    logger.info(f"Waiting for selector: {wait_selector}")
                    self.page.wait_for_selector(wait_selector, timeout=10000)
                
                # å¯é€‰ï¼šç­‰å¾…ç½‘ç»œç©ºé—²ï¼ˆå¤§å¤šæ•°æƒ…å†µä¸éœ€è¦ï¼‰
                if wait_for_network_idle:
                    try:
                        self.page.wait_for_load_state('networkidle', timeout=10000)
                    except Exception as e:
                        logger.debug(f"Network idle timeout (this is usually OK): {e}")
                else:
                    # ç®€å•ç­‰å¾…ä¸€ä¸‹è®©é¡µé¢æ¸²æŸ“
                    time.sleep(2)
                
                # ç¤¼è²Œæ€§å»¶è¿Ÿ
                time.sleep(self.delay)
                
                logger.info(f"âœ… Successfully loaded: {url}")
                return True
                
            except Exception as e:
                logger.warning(f"Failed to load {url} (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    wait_time = self.delay * (attempt + 2)
                    logger.info(f"Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Failed to load {url} after {max_retries} attempts")
                    return False
        
        return False
    
    def scroll_page(self, scroll_times: int = 3):
        """
        æ»šåŠ¨é¡µé¢ä»¥åŠ è½½åŠ¨æ€å†…å®¹
        
        Args:
            scroll_times: æ»šåŠ¨æ¬¡æ•°
        """
        if not self.page:
            return
        
        for i in range(scroll_times):
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(1)
            logger.debug(f"Scrolled page {i + 1}/{scroll_times}")
    
    def get_html(self) -> str:
        """
        è·å–å½“å‰é¡µé¢çš„ HTML
        
        Returns:
            HTML å†…å®¹
        """
        if not self.page:
            return ""
        return self.page.content()
    
    def download_file(self, url: str, save_path: Path) -> bool:
        """
        ä¸‹è½½æ–‡ä»¶
        
        Args:
            url: æ–‡ä»¶ URL
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨ page çš„ä¸‹è½½åŠŸèƒ½
            with self.page.expect_download() as download_info:
                self.page.goto(url)
            
            download = download_info.value
            download.save_as(save_path)
            
            file_size = FileUtils.get_file_size_mb(save_path)
            logger.info(f"Downloaded {url} to {save_path} ({file_size:.2f} MB)")
            return True
            
        except Exception as e:
            logger.error(f"Failed to download {url}: {e}")
            if save_path.exists():
                save_path.unlink()
            return False
    
    def take_screenshot(self, save_path: Path, full_page: bool = True):
        """
        æˆªå›¾
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            full_page: æ˜¯å¦å…¨é¡µæˆªå›¾
        """
        if not self.page:
            return
        
        try:
            save_path.parent.mkdir(parents=True, exist_ok=True)
            self.page.screenshot(path=str(save_path), full_page=full_page)
            logger.info(f"Screenshot saved to {save_path}")
        except Exception as e:
            logger.error(f"Failed to take screenshot: {e}")
    
    @abstractmethod
    def scrape(self, max_items: Optional[int] = None) -> List[Dict]:
        """
        æ‰§è¡Œçˆ¬å–ï¼ˆç”±å­ç±»å®ç°ï¼‰
        
        Args:
            max_items: æœ€å¤§çˆ¬å–æ•°é‡
            
        Returns:
            çˆ¬å–çš„æ•°æ®åˆ—è¡¨
        """
        pass
    
    def save_results(self, results: List[Dict], output_file: Path):
        """
        ä¿å­˜çˆ¬å–ç»“æœåˆ° JSON æ–‡ä»¶
        
        Args:
            results: ç»“æœåˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import json
        
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved {len(results)} results to {output_file}")
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        try:
            if self.page:
                self.page.close()
            if self.context:
                self.context.close()
            if self.browser:
                self.browser.close()
            if self.playwright:
                self.playwright.stop()
            
            logger.info(f"Closed Playwright scraper: {self.name}")
            
        except Exception as e:
            logger.error(f"Error closing browser: {e}")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.close()

